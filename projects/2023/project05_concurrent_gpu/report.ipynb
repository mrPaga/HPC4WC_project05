{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<div style='text-align: center;\n",
    "            width: 90%;\n",
    "            font-size: 20pt;'>\n",
    "<b>Asynchronous and concurrent execution on GPUs - Room for improvement in High Performance Computing for Weather and Climate Models?\n",
    "<br> <br></b>\n",
    "</div>\n",
    "<br><br><br><br>\n",
    "Melina Abeling, Julian Aeissen and Michele Pagani <br>\n",
    "High Performance Computing for Weather and Climate <br>\n",
    "Supervisor Oliver Fuhrer<br>\n",
    "Summer term 2023<br>\n",
    "melina.abeling@students.unibe.ch, julian.aeissen@students.unibe.ch, paganimi@student.ethz.ch\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "In order to better represent earth´s complex climate the resolution of climate models is becoming continuously higher which improves the skill of climate and weather models forecasts. In numerical weather predictions the grid consisting of cells or points, for example an icosahedron (ICON-model) sphere, is projected on the globe to discretize and solve the governing, often differential equations, for the whole grid size (Zängel et al., 2015, Wahib 2014). Associated with the higher spatial resolution is a need for higher computing power, as it means a finer grid consisting of more cells and points for which calculations have to be done in greater detail.\n",
    "Here, high performance computing and especially Graphical Process Units (GPU´s), hardware that accellerates computing also due to the ability to process multiple tasks in parallel, are important to meet the demand for better computing performance. \n",
    "One method commonly used to solve the necessary partial differential equations in climate models are stencil motivs (Schäfer & Fey, 2011; Wahib 2014).\n",
    "Those algorithms are space and time discrete that uniformly compute the value of a specific grid point based on its own value and those of its neigbouring grid points (Schäfer & Fey, 2011). \n",
    "This project aims to investigate how the maxmimum efficiency for various sized tasks, here the mentioned stencil motifs, differ for asynchronous and concurrent execution on GPUs or if it is even possible to reach a total utilization of the available hardware. For this a simple 2D Jacobian stencil and a Gaussian stencil will be utilized to investigate in which scenarios which execution leads to maximum efficiency or if maximum efficiency could not be reached.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "\n",
    "To investigate whether, or better, when asynchronous or concurrent execution on GPUs is most preferable, different scenarios where applied (Figure 1). The starting point for this is an initially unperturbed field (a square) or grid of fields to better be able to observe changes and correctness of computation (Figure 2). \n",
    "<br /> <br> \n",
    "<img src=\"images/Diagram-1.png\" alt=\"drawing\" width=\"600\"/>  <br>\n",
    "Figure 1: Project draft for asyncronous vs. concurrent execution on GPU´s. The y-axis represents the elapsed time after submitting tasks of various size.\n",
    "Therefore two stencils were chosen in order to see the influence they have on the preformance of GPU execution.\n",
    "\n",
    "![Initial Fields](images/InitialField.png) <br>\n",
    "Figure 2: Initial fields, whithout any blurr or perturbations.\n",
    "\n",
    "#### 2D Jacobi stencil\n",
    "The Jacobi Stencil calculates a weighted average of nearest neighbours and the center grid points and is therefore a five point stencil (Figure 3). In one time step two multiplication and 4 additions are performed. With double precision numbers at least 8 bytes have to be read or written (single-digit), yielding an Arithmetic Intensity of <0.5 FLOP/Byte. <br>\n",
    "<img src=\"images/JacobiStencil_Scheme.png\" alt=\"drawing\" width=\"255\"/>  <img src=\"images/EffectStencilA.png\" alt=\"drawing\" width=\"550\"/> <br>\n",
    "Figure 3: The illustration on the left shows a Jacobian Stencil. The figures on the right display the effect of 1000 iterations (blurr) on the field and over the whole grid. \n",
    "\n",
    "#### Gaussian 5x5 stencil\n",
    "The gaussian 5x5 stencil on the other hand is much larger, being a 25 point stencil (Figure 4). It is a discrete approximation of the 2D Gaussian filter/blurr. In one time step/grid update 24 FLOP are performed per grid point (addition or multiplication or addition followed by multiplication). Again at least one new number is written or read (with double precision) yielding an intensity of < 3 FLOP/Byte which can be considerably larger than for the smaller Jacobi stencil.<br>\n",
    "<img src=\"images/GaussStencilScheme.png\" alt=\"drawing\" width=\"300\"/>   <img src=\"images/EffectStencilB.png\" alt=\"drawing\" width=\"550\"/> <br>\n",
    "Figure 4: The left image shows an illustration of the Gaussian Stencil (5x5) and the right images portray the effect 1000 iterations (blurr) have on the field and the whole grid. \n",
    "\n",
    "#### GPU parallelization\n",
    "To investigate the impact on performance of different levels of concurrency on the GPU two approaches were applied. First, the difference in performance with different levels of parallelization was compared, i.e. the total field was divided into a varying number of tiles. Each tile is assigned a different stream on the GPU. In the second approach the field is also divided into tiles, but the execution of the tiles is now done sequentially via a for loop which represents different tasks for the CPU.\n",
    "<br>\n",
    "<img src=\"images/tiledGrid.png\" alt=\"drawing\" width=\"500\"/> <br>\n",
    "Figure 5: Sketch of the division of the fields into tiles, in parallel execution each tile is executed in one seperate stream by the GPU.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "\n",
    "### GPU versus Streams \n",
    "#### Performance over concurrency\n",
    "\n",
    "The streams come with overhead so dividing the field in tiles and computing each tile in a different stream leads, as expected, to a worse performance than having the whole field in a single stream. \n",
    "\n",
    "![Parallel exec](images/gpu_Parallel.png) <br>\n",
    "Figure 6: Execution time for parallel execution of one stream per tile. For the 5-point stencil (left) and the 25-point stencil (right).\n",
    "\n",
    "\n",
    "#### Performance over grid size\n",
    "\n",
    "Figures 7 and 8 show the execution time of 10 iterations with the Jacobi Stencil and the Gaussian stencil for different total field sizes. Since the curves flatten towards small grid sizes (up to field sizes of 1024x1024 the time stays almost constant over time), we can readily read off the overhead of the programm. The overhead becomes - unsurprisingly - bigger with more streams. The actual execution time of the stencil calculations only becomes relevant, depending on the number of streams, for grid sizes bigger than (1024x1024).\n",
    "\n",
    "\n",
    "![grid Size A](images/gpu_fieldSize.png) <br>\n",
    "Figure 7: Execution time vs different field sizes with the 5-point stencil for varying number of streams.\n",
    "\n",
    "![grid Size B](images/gpu_fieldSizeB.png)<br>\n",
    "Figure 8: Execution time vs different field sizes with the 25-point stencil for varying number of streams.\n",
    "\n",
    "### Fixed tiling\n",
    "#### Varying streams number\n",
    "Figure 9 shows the execution time averaged over 10 runs of the execution time, varying the number of streams. The tile size is kept constant at (256,256). As we can see from the two plots, as the number of streams increase, the performance slightly worsen. This is very probably because of the overhead of each stream, which is in line with the results of the previous section.\n",
    "\n",
    "![Parallel exec](images/tile_streams.png) <br>\n",
    "Figure 9: Execution time for parallel execution for fixed tile size (256,256) and varying number of streams. For the 5-point stencil (left) and the 25-point stencil (right).\n",
    "\n",
    "#### Varying tile size\n",
    "The size of the tiles dividing the field has virtual no influence, as it can be seen in Figures 10 and 11. Independently of the number of streams or the stencil, performance increases exponentially for increased tile size. This means the overhead for tiling and sending packets to the GPU is predominant here.\n",
    "![tile tileSizeA](images/tile_tileSizeA.png) <br>\n",
    "Figure 10: Execution time vs different number of streams with the 5-point stencil for varying tile sizes.\n",
    "\n",
    "![tile_tileSizeB](images/tile_tileSizeB.png)<br>\n",
    "Figure 11: Execution time vs different number of streams with the 25-point stencil for varying tile sizes.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "This work shows the performance for various-sized tasks and stencil motifs, using different approaches to parallelization: at GPU level (single call to GPU, single stream), at thread level (multiple calls to GPU, single stream), and at stream level. This was done both varying the overall problem size (first result section) and fixing it, but varying the size per GPU thread (second result section). In general, the best results were obtained by letting the GPU handle the parallelization, with as little external inputs such has forcing streams or tiles.\n",
    "<br>\n",
    "\n",
    "Indeed, the execution leaves room for improvement in HPC in Weather and Climate Modeling but it these very complex structures a perfect usage of hardware might not be possible.\n",
    "Further investigations on how these differences in efficiency can be utilized for different purposes in high performance computing for weather and climate modelling and It will be interesting to be able to witness how this might change with new hardware even higher computational power and the ever evolving code of Earth System Models.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "Alizadeh, O. (2022). Advances and challenges in climate modeling. Climatic Change, 170(1), 18. https://doi.org/10.1007/s10584-021-03298-4 \n",
    "<br> <br>\n",
    "Schäfer, A., & Fey, D. (2011). High performance stencil code algorithms for  GPGPUs [Proceedings of the International Conference on Computational Science, ICCS 2011]. Procedia Computer Science, 4, 2027–2036. https://doi.org/https://doi.org/10.1016/j.procs.2011.04.221\n",
    "<br> <br>\n",
    "Wahib, M., & Maruyama, N. (2014). Scalable kernel fusion for memory-bound GPU applications. SC ’14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 191–202. https://doi.org/10.1109/SC.2014.21\n",
    "<br> <br>\n",
    "Zängl, G., Reinert, D.,Rípodas, P., & Baldauf, M.(2015).The ICON(ICOsahedral Non-hydrostatic) modelling framework of DWD and MPI-M: Description of the non-hydrostatic dynamical core. Quarterly Journal of the Royal Meteorological Society, 141(687), 563–579. https://doi.org/10.1002/qj.2378\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "### Code snippets\n",
    "*For more information, please refer to `computation.ipypnb` and `utils.py`*\n",
    "#### Stencils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian stencil\n",
    "def jacobi_stencil_2d(in_field, out_field, n_halo, alpha=0.5, beta=0.125):\n",
    "    # Checks\n",
    "    assert len(in_field.shape) == 2\n",
    "    assert len(out_field.shape) == 2\n",
    "    h,w = out_field.shape\n",
    "    h_in_,w_in_ = in_field.shape\n",
    "    assert h_in_ == h + 2*n_halo\n",
    "    assert w_in_ == w + 2*n_halo\n",
    "    # IMPORTANT always have an expected halo\n",
    "    assert n_halo == 1\n",
    "\n",
    "    # Computation\n",
    "    out_field[:,:] = (\n",
    "        alpha * in_field[1:-1, 1:-1]\n",
    "        + beta * ( in_field[2:, 1:-1]+  in_field[:-2, 1:-1]\n",
    "        +  in_field[1:-1, 2:]+  in_field[1:-1, :-2] )\n",
    "    )\n",
    "    \n",
    "# Simple 5x5 Gaussian filter\n",
    "def gaussian_5x5_stencil_2d(in_field, out_field, n_halo):\n",
    "    # Checks\n",
    "    assert len(in_field.shape) == 2\n",
    "    assert len(out_field.shape) == 2\n",
    "    h,w = out_field.shape\n",
    "    h_in_,w_in_ = in_field.shape\n",
    "    assert h_in_ == h + 2*n_halo\n",
    "    assert w_in_ == w + 2*n_halo\n",
    "    \n",
    "    # IMPORTANT always have an expected halo\n",
    "    assert n_halo == 2\n",
    "\n",
    "    # Computation\n",
    "    out_field[:,:] = (\n",
    "        (\n",
    "            in_field[0:-4, 0:-4]\n",
    "            + 4.0 * in_field[0:-4, 1: -3]\n",
    "            + 6.0 * in_field[0:-4, 2: -2]\n",
    "            + 4.0 * in_field[0:-4, 3: -1]\n",
    "            + in_field[0:-4, 4: ]\n",
    "        )\n",
    "        + 4.0 * (\n",
    "            in_field[1:-3, 0:-4]\n",
    "            + 4.0 * in_field[1:-3, 1: -3]\n",
    "            + 6.0 * in_field[1:-3, 2: -2]\n",
    "            + 4.0 * in_field[1:-3, 3: -1]\n",
    "            + in_field[1:-3, 4: ]\n",
    "        )\n",
    "        + 6.0 * (\n",
    "            in_field[2:-2, 0:-4]\n",
    "            + 4.0 * in_field[2:-2, 1: -3]\n",
    "            + 6.0 * in_field[2:-2, 2: -2]\n",
    "            + 4.0 * in_field[2:-2, 3: -1]\n",
    "            + in_field[2:-2, 4: ]\n",
    "        )\n",
    "        + 4.0 * (\n",
    "            in_field[3:-1, 0:-4]\n",
    "            + 4.0 * in_field[3:-1, 1: -3]\n",
    "            + 6.0 * in_field[3:-1, 2: -2]\n",
    "            + 4.0 * in_field[3:-1, 3: -1]\n",
    "            + in_field[3:-1, 4: ]\n",
    "        )\n",
    "        + (\n",
    "            in_field[4:, 0:-4]\n",
    "            + 4.0 * in_field[4:, 1: -3]\n",
    "            + 6.0 * in_field[4:, 2: -2]\n",
    "            + 4.0 * in_field[4:, 3: -1]\n",
    "            + in_field[4:, 4: ]\n",
    "        )\n",
    "    ) / 256.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian stencil\n",
    "def jacobi_stencil_2d(in_field, out_field, n_halo, alpha=0.5, beta=0.125):\n",
    "    # Checks\n",
    "    assert len(in_field.shape) == 2\n",
    "    assert len(out_field.shape) == 2\n",
    "    h,w = out_field.shape\n",
    "    h_in_,w_in_ = in_field.shape\n",
    "    assert h_in_ == h + 2*n_halo\n",
    "    assert w_in_ == w + 2*n_halo\n",
    "    # IMPORTANT always have an expected halo\n",
    "    assert n_halo == 1\n",
    "\n",
    "    # Computation\n",
    "    out_field[:,:] = (\n",
    "        alpha * in_field[1:-1, 1:-1]\n",
    "        + beta * ( in_field[2:, 1:-1]+  in_field[:-2, 1:-1]\n",
    "        +  in_field[1:-1, 2:]+  in_field[1:-1, :-2] )\n",
    "    )\n",
    "    \n",
    "# Simple 5x5 Gaussian filter\n",
    "def gaussian_5x5_stencil_2d(in_field, out_field, n_halo):\n",
    "    # Checks\n",
    "    assert len(in_field.shape) == 2\n",
    "    assert len(out_field.shape) == 2\n",
    "    h,w = out_field.shape\n",
    "    h_in_,w_in_ = in_field.shape\n",
    "    assert h_in_ == h + 2*n_halo\n",
    "    assert w_in_ == w + 2*n_halo\n",
    "    \n",
    "    # IMPORTANT always have an expected halo\n",
    "    assert n_halo == 2\n",
    "\n",
    "    # Computation\n",
    "    out_field[:,:] = (\n",
    "        (\n",
    "            in_field[0:-4, 0:-4]\n",
    "            + 4.0 * in_field[0:-4, 1: -3]\n",
    "            + 6.0 * in_field[0:-4, 2: -2]\n",
    "            + 4.0 * in_field[0:-4, 3: -1]\n",
    "            + in_field[0:-4, 4: ]\n",
    "        )\n",
    "        + 4.0 * (\n",
    "            in_field[1:-3, 0:-4]\n",
    "            + 4.0 * in_field[1:-3, 1: -3]\n",
    "            + 6.0 * in_field[1:-3, 2: -2]\n",
    "            + 4.0 * in_field[1:-3, 3: -1]\n",
    "            + in_field[1:-3, 4: ]\n",
    "        )\n",
    "        + 6.0 * (\n",
    "            in_field[2:-2, 0:-4]\n",
    "            + 4.0 * in_field[2:-2, 1: -3]\n",
    "            + 6.0 * in_field[2:-2, 2: -2]\n",
    "            + 4.0 * in_field[2:-2, 3: -1]\n",
    "            + in_field[2:-2, 4: ]\n",
    "        )\n",
    "        + 4.0 * (\n",
    "            in_field[3:-1, 0:-4]\n",
    "            + 4.0 * in_field[3:-1, 1: -3]\n",
    "            + 6.0 * in_field[3:-1, 2: -2]\n",
    "            + 4.0 * in_field[3:-1, 3: -1]\n",
    "            + in_field[3:-1, 4: ]\n",
    "        )\n",
    "        + (\n",
    "            in_field[4:, 0:-4]\n",
    "            + 4.0 * in_field[4:, 1: -3]\n",
    "            + 6.0 * in_field[4:, 2: -2]\n",
    "            + 4.0 * in_field[4:, 3: -1]\n",
    "            + in_field[4:, 4: ]\n",
    "        )\n",
    "    ) / 256.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an empty initial field with a square of size / 2 at the center of value `value`\n",
    "def get_initial_field_square(size, n_halo, value = 1.0) -> cp.ndarray:\n",
    "    # Check parameters\n",
    "    assert type(size) == tuple\n",
    "    dim = len(size)\n",
    "    # assert dim in [2,3]\n",
    "    assert dim in [2]\n",
    "\n",
    "    # Init\n",
    "    h, w = size[-2], size[-1]\n",
    "\n",
    "    # Add halo\n",
    "    h += 2*n_halo\n",
    "    w += 2*n_halo\n",
    "\n",
    "    # 2d\n",
    "    if dim == 2:\n",
    "        field = cp.zeros((h, w), dtype=cp.float32)\n",
    "        field[ h//4 : 3*h//4,\n",
    "               w//4 : 3*w//4 ] = value\n",
    "\n",
    "    # 3d\n",
    "    elif dim == 3:\n",
    "        field = cp.zeros((size[0], h, w), dtype=cp.float32)\n",
    "        field[ :,\n",
    "               h//4 : 3*h//4,\n",
    "               w//4 : 3*w//4 ] = value\n",
    "\n",
    "    return field\n",
    "\n",
    "# Creates an grid of spacing `spacing`\n",
    "def get_initial_field_grid(size, n_halo, value = 1.0, spacing=50) -> cp.ndarray:\n",
    "    # Check parameters\n",
    "    assert type(size) == tuple\n",
    "    dim = len(size)\n",
    "    # assert dim in [2,3]\n",
    "    assert dim in [2]\n",
    "\n",
    "    # Init\n",
    "    h, w = size[-2], size[-1]\n",
    "\n",
    "    # Add halo\n",
    "    h += 2*n_halo\n",
    "    w += 2*n_halo\n",
    "\n",
    "    # 2d\n",
    "    if dim == 2:\n",
    "        field = cp.zeros((h, w), dtype=cp.float32)\n",
    "\n",
    "        # Horizontal strides\n",
    "        for i in range(h // spacing + 2):\n",
    "            if i * spacing >= h:\n",
    "                break\n",
    "            idx = spacing // 3 + i * spacing\n",
    "            field[ idx : idx + spacing // 3, :] = value\n",
    "            \n",
    "        # Vertical strides\n",
    "        for i in range(w // spacing + 2):\n",
    "            if i * spacing >= w:\n",
    "                break\n",
    "            idx = spacing // 3 + i * spacing\n",
    "            field[ :, idx : idx + spacing // 3] = value\n",
    "\n",
    "    # 3d\n",
    "    elif dim == 3:\n",
    "        field = cp.zeros((size[0], h, w), dtype=cp.float32)\n",
    "        \n",
    "        # Horizontal strides\n",
    "        for i in range(h // spacing + 2):\n",
    "            if i * spacing >= h:\n",
    "                break\n",
    "            idx = spacing // 3 + i * spacing\n",
    "            field[ :, idx : idx + spacing // 3, :] = value\n",
    "            \n",
    "        # Vertical strides\n",
    "        for i in range(w // spacing + 2):\n",
    "            if i * spacing >= w:\n",
    "                break\n",
    "            idx = spacing // 3 + i * spacing\n",
    "            field[ :, :, idx : idx + spacing // 3] = value\n",
    "        \n",
    "    return field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiling and GPU computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "def compute_gpu_2d(in_field, stencil, n_stream, n_iter, n_halo, tile_size = None):\n",
    "    # Init\n",
    "    out_field = cp.copy(in_field)\n",
    "    \n",
    "    # Check in_field\n",
    "    dim = len(in_field.shape)\n",
    "    assert dim == 2\n",
    "    h,w = in_field.shape\n",
    "    h -= 2*n_halo\n",
    "    w -= 2*n_halo\n",
    "    \n",
    "    # Chech force tile_size\n",
    "    if tile_size is None:\n",
    "        assert math.sqrt(n_stream).is_integer()\n",
    "        tiles_per_side = (int(math.sqrt(n_stream)), int(math.sqrt(n_stream)))\n",
    "        h_tile = h // tiles_per_side[0]\n",
    "        w_tile = w // tiles_per_side[1]\n",
    "    else:\n",
    "        assert h % tile_size[0] == 0\n",
    "        assert w % tile_size[1] == 0\n",
    "        h_tile, w_tile = tile_size\n",
    "        tiles_per_side = (h // tile_size[0], w // tile_size[1])\n",
    "\n",
    "        \n",
    "    # Create streams\n",
    "    streams = [ cp.cuda.Stream() for _ in range(n_stream) ]\n",
    "\n",
    "    for iter in range(n_iter):\n",
    "        # Init\n",
    "        e = cp.cuda.Event()\n",
    "        e.record()\n",
    "\n",
    "        update_halo(in_field, n_halo)\n",
    "\n",
    "        # Iterate over tiles\n",
    "        for i in range(tiles_per_side[0]):\n",
    "            for j in range(tiles_per_side[1]):\n",
    "                # # Indeces\n",
    "                idx_s = (i*tiles_per_side[0] + j) % n_stream\n",
    "                with streams[idx_s]:\n",
    "                    # Stencil iteration\n",
    "                    stencil(\n",
    "                        in_field[\n",
    "                            i*h_tile: 2*n_halo + (i+1)*h_tile,\n",
    "                            j*w_tile: 2*n_halo + (j+1)*w_tile\n",
    "                        ],\n",
    "                        out_field[\n",
    "                            n_halo + i*h_tile: n_halo + (i+1)*h_tile,\n",
    "                            n_halo + j*w_tile: n_halo + (j+1)*w_tile\n",
    "                        ],\n",
    "                        n_halo\n",
    "                    )\n",
    "\n",
    "        # Syncronize all streams\n",
    "        e.synchronize()\n",
    "\n",
    "        # Update out_field\n",
    "        if iter < n_iter - 1:\n",
    "            in_field, out_field = out_field, in_field\n",
    "            \n",
    "    return out_field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
