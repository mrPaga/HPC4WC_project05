{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<div style='text-align: center;\n",
    "            width: 90%;\n",
    "            font-size: 20pt;'>\n",
    "    <b>Asynchronous and concurrent execution on GPUs - \n",
    "        <br> <br> XXXXX?</b>\n",
    "</div>\n",
    "<br><br><br><br>\n",
    "Melina Abeling, Julian Aeissen and Michele Pagani <br>\n",
    "High Performance Computing for Weather and Climate <br>\n",
    "Supervisor Oliver Fuhrer<br>\n",
    "Summer term 2023<br>\n",
    "melina.abeling@students.unibe.ch, paganimi@student.ethz.ch\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "In order to better represent earth´s complex climate the resolution of climate models is becoming continuously higher which improves the skill of climate and weather models forecasts. In numerical weather predictions the grid consisting of cells or points, for example an icosahedron (ICON-model) sphere, is projected on the globe to discretize and solve the governing, often differential equations, for the whole grid size (Zängel et al., 2015, Wahib 2014). Associated with the higher spatial resolution is a need for higher computing power, as it means a finer grid consisting of more cells and points for which calculations have to be done in greater detail.\n",
    "Here, high performance computing and especially Graphical Process Units (GPU´s), hardware that accellerates computing also due to the ability to process multiple tasks in parallel, are important to meet the demand for better computing performance. \n",
    "One method commonly used to solve the necessary partial differential equations in climate models are stencil motivs (Schäfer & Fey, 2011; Wahib 2014).\n",
    "Those algorithms are space and time discrete that uniformly compute the value of a specific grid point based on its own value and those of its neigbouring grid points (Schäfer & Fey, 2011). \n",
    "This project aims to investigate how the maxmimum efficiency for various sized tasks, here the mentioned stencil motifs, differ for asynchronous and concurrent execution on GPUs or if it is even possible to reach a total utilization of the available hardware. For this a simple 2D Jacobian stencil and a Gaussian stencil will be utilized to investigate in which scenarios which execution leads to maximum efficiency or if maximum efficiency could not be reached.\n",
    "</div>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "\n",
    "To investigate whether, or better, when asynchronous or concurrent execution on GPUs is most preferable, different scenarios where applied (Figure 1). The starting point for this is an initially unperturbed field (a square) or grid of fields to better be able to observe changes and correctness of computation (Figure 2). \n",
    "<br /> <br> \n",
    "<img src=\"images/Diagram-1.png\" alt=\"drawing\" width=\"700\"/>  <br>\n",
    "Figure 1: Project draft for asyncronous vs. concurrent execution on GPU´s. The y-axis represents the elapsed time after submitting tasks of various size.\n",
    "\n",
    "![Initial Fields](images/InitialField.png) <br>\n",
    "Figure 2: This figure shows how the fields look initially, whithout any blurr or perturbations.\n",
    "\n",
    "\n",
    "Therefore two stencils were chosen in order to see the influence they have on the preformance of GPU execution.\n",
    "\n",
    "#### 2D Jacobi stencil\n",
    "The Jacobi Stencil calculates a weighted average of nearest neighbours and the center grid points and is therefore a five point stencil (Figure 3). In one time step two multiplication and 4 additions are performed. With double precision numbers at least 8 bytes have to be read or written (single-digit), yielding an Arithmetic Intensity of <0.5 FLOP/Byte. <br>\n",
    "<img src=\"images/JacobiStencil_Scheme.png\" alt=\"drawing\" width=\"255\"/>  <img src=\"images/EffectStencilA.png\" alt=\"drawing\" width=\"550\"/> <br>\n",
    "Figure 3: The illustration on the left shows a Jacobian Stencil. The figures on the right display the effect of 1000 iterations (blurr) on the field and over the whole grid. \n",
    "\n",
    "#### Gaussian 5x5 stencil\n",
    "The gaussian 5x5 stencil on the other hand is much larger, being a 25 point stencil (Figure 4). It is a discrete approximation of the 2D Gaussian filter/blurr. In one time step/grid update 24 FLOP are performed per grid point (addition or multiplication or addition followed by multiplication). Again at least one new number is written or read (with double precision) yielding an intensity of < 3 FLOP/Byte which can be considerably larger than for the smaller Jacobi stencil.<br>\n",
    "<img src=\"images/GaussStencilScheme.png\" alt=\"drawing\" width=\"300\"/>   <img src=\"images/EffectStencilB.png\" alt=\"drawing\" width=\"550\"/> <br>\n",
    "Figure 4: The left image shows an illustration of the Gaussian Stencil (5x5) and the right images portray the effect 1000 iterations (blurr) have on the field and the whole grid. \n",
    "\n",
    "#### GPU parallelization\n",
    "To investigate the impact on performance of different levels of concurrency on the GPU two approaches were applied. First, the difference in performance with different levels of parallelization was compared, i.e. the total field was divided into a varying number of tiles. Each tile is assigned a different stream on the GPU. In the second approach the field is also divided into tiles, but the execution of the tiles is now done sequentially via a for loop which represents different tasks for the CPU.\n",
    "<br>\n",
    "<img src=\"images/tiledGrid.png\" alt=\"drawing\" width=\"500\"/> <br>\n",
    "Figure 5: \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "\n",
    "### GPU versus Streams \n",
    "#### Performance over concurrency\n",
    "\n",
    "The streams come with overhead so dividing the field in tiles and computing each tile in a different stream is leads, as expected, to a worse performance than having the whole field in a single stream. \n",
    "\n",
    "![Parallel exec](images/gpu_Parallel.png) <br>\n",
    "Figure 6: \n",
    "\n",
    "\n",
    "#### Performance over grid size\n",
    "\n",
    "Figure XXXXXX shows the execution time of 10 iterations with the Jacobi Stencil and the Gaussian stencil for different total field sizes. Since The curves flatten towards small grid sizes (up to field sizes of 1024x1024 the time stays almost constant over time), we can readily read off the overhead of the programm. The overhead becomes -unsurprisingly- bigger with more streams. The actual execution time of the stencil calculations dominates/becomes relevant, depending on the number of streams, only for grid sizes bigger than (1024x1024). Therefore\n",
    "\n",
    "\n",
    "![grid Size A](images/gpu_fieldSize.png) <br>\n",
    "Figure 7: \n",
    "\n",
    "![grid Size B](images/gpu_fieldSizeB.png)<br>\n",
    "Figure 8:\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "This work demonstrated how \n",
    "\n",
    "<br>    <br> \n",
    "\n",
    "Further investigations on how \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "Alizadeh, O. (2022). Advances and challenges in climate modeling. Climatic Change, 170(1), 18. https://doi.org/10.1007/s10584-021-03298-4 \n",
    "<br> <br>\n",
    "Schäfer, A., & Fey, D. (2011). High performance stencil code algorithms for  GPGPUs [Proceedings of the International Conference on Computational Science, ICCS 2011]. Procedia Computer Science, 4, 2027–2036. https://doi.org/https://doi.org/10.1016/j.procs.2011.04.221\n",
    "<br> <br>\n",
    "Wahib, M., & Maruyama, N. (2014). Scalable kernel fusion for memory-bound GPU applications. SC ’14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 191–202. https://doi.org/10.1109/SC.2014.21\n",
    "<br> <br>\n",
    "Zängl, G., Reinert, D.,Rípodas, P., & Baldauf, M.(2015).The ICON(ICOsahedral Non-hydrostatic) modelling framework of DWD and MPI-M: Description of the non-hydrostatic dynamical core. Quarterly Journal of the Royal Meteorological Society, 141(687), 563–579. https://doi.org/10.1002/qj.2378\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
