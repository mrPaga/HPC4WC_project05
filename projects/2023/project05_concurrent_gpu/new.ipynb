{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous and concurrent execution on GPUs\n",
    ">*Melina Abeling, Julian Aeissen and Michele Pagani*. Supervised by *Oliver Fuhrer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "In order to better represent earth´s complex climate the resolution of climate models is becoming continuously higher which improves the skill of climate and weather models forecasts. In numerical weather predictions the grid consisting of cells or points, for example an icosahedron (ICON-model) sphere, is projected on the globe to discretize and solve the governing, often differential equations, for the whole grid size (Zängel et al., 2015, Wahib 2014). Associated with the higher spatial resolution is a need for higher computing power, as it means a finer grid consisting of more cells and points for which calculations have to be done in greater detail.\n",
    "Here, high performance computing and especially Graphical Process Units (GPU´s), hardware that accellerates computing also due to the ability to process multiple tasks in parallel, are important to meet the demand for better computing performance. \n",
    "One method commonly used to solve the necessary partial differential equations in climate models are stencil motivs (Schäfer & Fey, 2011; Wahib 2014).\n",
    "Those algorithms are space and time discrete that uniformly compute the value of a specific grid point based on its own value and those of its neigbouring grid points (Schäfer & Fey, 2011).\n",
    "<br> <br>\n",
    "XXXXXX\n",
    "<br> <br>\n",
    "This project aims to investigate how the maxminum efficiency for various sized tasks differ for asynchronous and concurrent execution on GPUs or if it is even possible to reach a total utilization of the available hardware. For this a simple 2D Jacobian stencil and a Gaussian stencil will be utilized to investigate in which scenarios which execution leads to maximum efficiency/ or maximum efficiency could not be reached.\n",
    "<br> <br>\n",
    "XXXXXXXX\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Initial fields\n",
    "*TODO*\n",
    "> square and grid, to better check the stencil and computations correctness\n",
    "\n",
    "![Initial Fields](images/InitialField.png)\n",
    "\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "    \n",
    "To investigate whether or better when asynchronous or concurrent execution on GPUs is most preferable, different scenarios where applied. Therefore two stencils of different size in order to see the influence they have on the preformance of GPU execution (see DIAGRAM). \n",
    "\n",
    "### PICTURE\n",
    "\n",
    "#### 2D Jacobi stencil\n",
    "The Jacobi Stencil calculates a weighted average of nearest neighbours and the center grid points and is therefore a five point stencil. In one time step two multiplication and 4 additions are performed. With double precision numbers at least 8 bytes have to be read or written (single-digit), yielding an Arithmetic Intensity of <0.5 FLOP/Byte.\n",
    "\n",
    "![Jacobi](images/JacobiStencil_Scheme.png)\n",
    "\n",
    "### PICTURE\n",
    "\n",
    "#### Effect of the Jacobi Stencil after 1000 iterations\n",
    "\n",
    "![Effect Jacobi](images/EffectStencilA.png)\n",
    "\n",
    "\n",
    "#### Gaussian 5x5 stencil\n",
    "The gaussian 5x5 stencil on the other hand is much larger, being a 25 point stencil. It is a discrete approximation of the 2D Gaussian filter/blurr. In one time step/grid update 24 FLOP are performed per grid point (addition or multiplication or addition followed by multiplication). Again at least one new number is written or read (with double precision) yielding an intensity of < 3 FLOP/Byte which can be considerably larger than for the smaller Jacobi stencil.\n",
    "\n",
    "![Gauss Scheme](images/GaussStencilScheme.png)\n",
    "\n",
    "#### Effect of the Gaussian blurr after 1000 iterations\n",
    "\n",
    "![Effect Jacobi](images/EffectStencilB.png)\n",
    "\n",
    "\n",
    "#### GPU parallelization\n",
    "\n",
    "To investigate the impact on performance of different levels of concurrency on the GPU two approaches were applied. First, the difference in performance with different levels of parallelization was compared, i.e. divide the total field into a varying number of tiles. Each tile is assigned a different stream on the GPU. In the second approach again the field is divided into tiles, but the execution of the tiles is now done sequentially via a for loop which represents different tasks for the CPU.\n",
    "\n",
    "tiling the field in equal portions analyzing 2 different things:\n",
    "\n",
    "difference in performance with different levels of parallelization (GPU level vs Stream level) ( 1 tile per stream)\n",
    "parallization of multiple independent tasks with streams (fixed number of tiles, different subdivision of them in the streams)\n",
    "\n",
    "<img src=\"images/tiledGrid.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "\n",
    "### GPU vs Streams \n",
    "#### Performance over concurrency\n",
    "\n",
    "\n",
    "As expected, streams come with overhead so dividing the field in tiles and computing each tile in a different stream is leads to a worse performance than having the whole field in a single stream. \n",
    "\n",
    "![Parallel exec](images/gpu_Parallel.png)\n",
    "\n",
    "\n",
    "#### Performance over grid size\n",
    "\n",
    "Figure XXXXXX shows the execution time of 10 iterations with the Jacobi Stencil and the Gaussian stencil for different total field sizes. Since The curves flatten towards small grid sizes (up to field sizes of 1024x1024 the time stays almost constant over time), we can readily read off the overhead of the programm. The overhead becomes -unsurprisingly- bigger with more streams. The actual execution time of the stencil calculations dominates/becomes relevant, depending on the number of streams, only for grid sizes bigger than (1024x1024). Therefore\n",
    "\n",
    "\n",
    "![grid Size A](images/gpu_fieldSize.png)\n",
    "![grid Size B](images/gpu_fieldSizeB.png)\n",
    "\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "This work demonstrated how \n",
    "\n",
    "<br>    <br> \n",
    "\n",
    "Further investigations on how \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align: justify;\n",
    "            width: 90%;'>\n",
    "Alizadeh, O. (2022). Advances and challenges in climate modeling. Climatic Change, 170(1), 18. https://doi.org/10.1007/s10584-021-03298-4 \n",
    "<br> <br>\n",
    "Schäfer, A., & Fey, D. (2011). High performance stencil code algorithms for  GPGPUs [Proceedings of the International Conference on Computational Science, ICCS 2011]. Procedia Computer Science, 4, 2027–2036. https://doi.org/https://doi.org/10.1016/j.procs.2011.04.221\n",
    "<br> <br>\n",
    "Wahib, M., & Maruyama, N. (2014). Scalable kernel fusion for memory-bound GPU applications. SC ’14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 191–202. https://doi.org/10.1109/SC.2014.21\n",
    "<br> <br>\n",
    "Zängl, G., Reinert, D.,Rípodas, P., & Baldauf, M.(2015).The ICON(ICOsahedral Non-hydrostatic) modelling framework of DWD and MPI-M: Description of the non-hydrostatic dynamical core. Quarterly Journal of the Royal Meteorological Society, 141(687), 563–579. https://doi.org/10.1002/qj.2378\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
